{"name":"GPc","tagline":"Gaussian process code in C++ including some implementations of GP-LVM and IVM.","body":"GPc\r\n===\r\n\r\nGaussian process code in C++ including some implementations of GP-LVM and IVM.\r\n\r\n\r\nGaussian Process Software\r\n=========================\r\n\r\nThis page describes how to compile and gives some examples of use of the C++ Gaussian Process code. \r\n\r\n### Release Information\r\n\r\nCurrent release is 0.001.\r\n\r\n### Design Philosophy\r\n\r\nThe software is written in C++ to try and get a degree of flexibility in the models that can be used without a serious performance hit. This was difficult to do in MATLAB as users who have tried version 1 (which was fast but inflexible) and version 2 (which was flexible but slow) of the MATLAB software will appreciate.\r\n\r\nThe software is mainly written in C++ but relies for some functions on FORTRAN code by other authors and the LAPACK and BLAS libraries. \r\n\r\nAs well as the C++ code some utilities are supplied in the corresponding MATLAB code for visualising the results. \r\n\r\n## Compiling the Software\r\n\r\nThe software was written with gcc on ubuntu.\r\n\r\nPart of the reason for using gcc is the ease of interoperability with FORTRAN. The code base makes fairly extensive use of FORTRAN so you need to have g77 installed.\r\nThe software is compiled by writing \r\n\r\n```sh\r\n$ make gp\r\n```\r\n\r\nat the command line. Architecture specific options are included in the `make.ARCHITECTURE` files. Rename the file with the relevant architecture to `make.inc` for it to be included.\r\n\r\n### Optimisation\r\n\r\nOne of the advantages of interfacing to the LAPACK and BLAS libraries is that they are often optimised for particular architectures. \r\n\r\n\r\n## General Information\r\n\r\nThe way the software operates is through the command line. There is one executable, `gp`. Help can be obtained by writing \r\n\r\n```sh\r\n$ ./gp -h\r\n```\r\n\r\nwhich lists the commands available under the software. Help for each command can then be obtained by writing, for example, \r\n\r\n```sh\r\n$ ./gp learn -h\r\n```\r\n\r\nAll the tutorial optimisations suggested take less than 1/2 hour to run on my less than 2GHz Pentium IV machine. The first oil example runs in a couple of minutes. Below I suggest using the highest verbosity options `-v 3` in each of the examples so that you can track the iterations.\r\n\r\n## Examples\r\n\r\nThe software loads in data in the <a href=\"http://svmlight.joachims.org/\">SVM light</a> format. This is to provide compatibility with other <a href=\"https://github.com/SheffieldML/GPmat/\">Gaussian Process software</a>. Anton Schwaighofer has written <a href=\"http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/software/svml_toolbox.html\"> a package</a> which can write from MATLAB to the SVM light format.\r\n\r\n## One Dimensional Data Data\r\n\r\n\r\nProvided with the software, in the `examples` directory, is a one dimensional regression problem. The file is called `spgp1d.svml`. \r\n\r\nFirst we will learn the data using the following command,\r\n\r\n```sh\r\n$ ./gp -v 3 learn -# 100 examples/sinc.svml sinc.model\r\n```\r\n\r\nThe flag `-v 3` sets the verbosity level to 3 (the highest level) which causes the iterations of the scaled conjugate gradient algorithm to be shown. The flag `-# 100` terminates the optimisation after 100 iterations so that you can quickly move on with the rest of the tutorial.\r\n\r\nThe software will load the data in `sinc.svml`. The labels are included in this file but they are <i>not</i> used in the optimisation of the model. They are for visualisation purposes only.\r\n\r\n### Gnuplot\r\n\r\nThe learned model is saved in a file called `sinc.model`. This file has a plain text format to make it human readable. Once training is complete, the learned covariance function parameters of the model can be displayed using \r\n\r\n\r\n```sh\r\n$ ./gp display sinc.model\r\n```\r\n\r\n```sh\r\nLoading model file.\r\n... done.\r\nStandard GP Model: \r\nOptimiser: scg\r\nData Set Size: 40\r\nKernel Type: \r\nScales learnt: 0\r\nX learnt: 0\r\nBias: 0.106658 \r\n\r\nScale: 1 \r\n\r\nGaussian Noise: \r\nBias on process 0: 0\r\nVariance: 1e-06\r\ncompound kernel:\r\nrbfinverseWidth: 0.198511\r\nrbfvariance: 0.0751124\r\nbiasvariance: 1.6755e-05\r\nwhitevariance: 0.00204124\r\n```\r\n\r\nNotice the fact that the covariance function is composed of an RBF kernel, also known as squared exponential kernel or Gaussian kernel; a bias kernel, which is just a constant, and a white noise kernel, which is a diagonal term. This is the default setting, it can be changed with flags to other covariance function types, see `./gp learn -h` for details.\r\n\r\nFor your convenience a `gnuplot` file may generated to visualise the data. First run\r\n\r\n```sh\r\n$ ./gp gnuplot -r 400 examples/sinc.svml sinc.model sinc\r\n```\r\n\r\nThe `sinc` supplied as the last argument acts as a stub for gnuplot to create names from, so for example (using gnuplot vs 4.0 or above), you can write\r\n\r\n```sh\r\n$ gnuplot sinc_plot.gp\r\n```\r\n\r\nNote: for this to work on OSX you may have to \r\n\r\n```sh\r\n$ brew install gnuplot --with-x\r\n```\r\n\r\nThen you should obtain the plot shown below\r\n<center><img src=\"sinc.png\"><br>\r\nGaussian process applied to sinc data.</center><br>\r\n\r\nThe other files created are `sinc_error_bar_data.dat`, which produces the error bars and `sinc_line_data.dat` which produces the mean as well as `sinc_scatter_data.dat` which shows the training data.\r\n\r\n### Other Data\r\n\r\nYou might also want to try a larger data set.\r\n\r\n```sh\r\n$ ./gp -v 3 learn -# 100 examples/spgp1d.svml spgp1d.model\r\n```\r\n\r\n### MATLAB and OCTAVE\r\n\r\nWhile MATLAB can be slow (and very expensive for non-academic users) it can still be a lot easier to code the visualisation routines by building on MATLAB's graphics facilities. To this end you can load in the results from the MATLAB/OCTAVE GPmat toolbox for further manipulation. You can download the toolbox from <a href=\"https://github.com/SheffieldML/GPmat/\">here</a>. Once the relevant toolboxes (you need all the dependent toolboxes) are downloaded you can visualise the results in MATLAB using\r\n\r\n```matlab\r\n>> [y, X] = svmlread('sinc.svml')\r\n>> gpReadFromFile('sinc.model', X, y)\r\n>>\r\n```\r\n\r\nwhere we have used the <a href=\"http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/software/svml_toolbox.html\">SVML toolbox</a> of Anton Schwaighofer to load in the data.\r\n\r\nIVM Software\r\n============\r\n\r\nThis page describes how to compile and gives some examples of use of the C++ Informative Vector Machine Software (IVM).\r\n\r\n### Design Philosophy\r\n\r\nThe software is written in C++ to try and get a degree of flexibility in the models that can be used without a serious performance hit. \r\n\r\nThe software is mainly written in C++ but relies for some functions on FORTRAN code by other authors and the LAPACK and BLAS libraries. \r\n\r\n## Compiling the Software\r\n\r\nThe software was written with gcc vs 3.2.2. There are definitely Standard Template Library issues on Solaris with gcc 2.95, so I suggest that at least version 3.2 or above is used.\r\n\r\nPart of the reason for using gcc is the ease of interoperability with FORTRAN. The code base makes fairly extensive use of FORTRAN so you need to have g77 installed.\r\nThe software is compiled by writing \r\n\r\n```sh\r\n$ make ivm\r\n```\r\n\r\nat the command line. Architecture specific options are included in the `make.ARCHITECTURE` files. Rename the file with the relevant architecture to `make.inc` for it to be included.\r\n\r\n### Optimisation\r\n\r\nOne of the advantages of interfacing to the LAPACK and BLAS libraries is that they are often optimised for particular architectures. The file `make.atlas` includes options for compiling the ATLAS optimised versions of lapack and blas that are available on a server I have access to. These options may vary for particular machines.\r\n\r\n### Cygwin\r\n\r\nFor Windows users the code compiles under cygwin. However you will need version s of the lapack and blas libraries available (see <a href=\"http://www.netlib.org\">www.netlib.org</a>). This can take some time to compile, and in the absence of any pre-compiled versions on the web I've provided some pre-compiled versions you may want to make use of (see the cygwin directory). Note that these pre-compiled versions are <i>not</i> optimised for the specific architecture and therefore do not give the speed up you would hope for from using lapack and blas.\r\n\r\n### Microsoft Visual C++\r\n\r\nAs of Release 0.101 the code compiles under Microsoft Visual Studio 7.1. A project file is provided in the current release in the directory `MSVC/ivm`. The compilation makes use of f2c versions of the FORTRAN code and the C version of LAPACK/BLAS, CLAPACK. Detailed instructions on how to compile are in the readme.msvc file. Much of the work to convert the code (which included ironing out several bugs) was done by William V. Baxter for the GPLVM code. \r\n\r\n## General Information\r\n\r\nThe way the software operates is through the command line. There is one executable, `ivm`. Help can be obtained by writing \r\n\r\n```sh\r\n$ ./ivm -h\r\n```\r\n\r\nwhich lists the commands available under the software. Help for each command can then be obtained by writing, for example, \r\n\r\n```sh\r\n$ ./ivm learn -h\r\n```\r\n\r\nAll the tutorial optimisations are suggested take less than 1/2 hour to run on my less than 2GHz Pentium IV machine. The first oil example runs in a couple of minutes. Below I suggest using the highest verbosity options `-v 3` in each of the examples so that you can track the iterations.\r\n\r\n## Bugs\r\n\r\nVictor Cheng writes:\r\n\r\n<i>\" ... I've tested your IVM C++ Gaussian Process tool (IVMCPP0p12 version). It is\r\nquite useful. However, the gnuplot function seems has a problem. Every time\r\nI type the command: \"Ivm gnuplot traindata name.model\", an error comes out\r\nas: \"Unknown noise model!\". When I test this function with IVMCPP0p11 IVM,\r\nits fine, but IVMCPP0p11 has another problem that it gives \"out of memory\"\r\nerror in test mode! So I use two vesions simultaneously. \"</i> \r\n\r\nI'm working (as of 31/12/2007) on a major rewrite, so it's unlikely that these bugs will be fixed in the near future, however if anyone makes a fix I'll be happy to incorporate it! Please let me know.\r\n\r\n\r\n## Examples\r\n\r\nThe software loads in data in the <a href=\"http://svmlight.joachims.org/\">SVM light</a> format. Anton Schwaighofer has written <a href=\"http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/software/svml_toolbox.html\"> a package</a> which can write from MATLAB to the SVM light format.\r\n\r\n## Toy Data Sets\r\n\r\nIn this section we present some simple examples. The results will be visualised using `gnuplot`. It is suggested that you have access to `gnuplot` vs 4.0 or above.\r\n\r\nProvided with the software, in the `examples` directory, are some simple two dimensional problems. We will first try classification with these examples.\r\n\r\nThe first example is data sampled from a Gaussian process with an RBF kernel function with inverse width of 10. The input data is sampled uniformly from the unit square. This data can be learnt with the following command.\r\n\r\n```sh\r\n$ ./ivm -v 3 learn -a 200 -k rbf examples/unitsquaregp.svml unitsquaregp.model\r\n```\r\n\r\nThe flag `-v 3` sets the verbosity level to 3 (the highest level) which causes the iterations of the scaled conjugate gradient algorithm to be shown. The flag `-a 200` sets the active set size. The kernel type is selected with the flag `-k rbf`. \r\n\r\n### Gnuplot\r\n\r\nThe learned model is saved in a file called `unitsquaregp.model`. This file has a plain text format to make it human readable. Once training is complete, the learned kernel parameters of the model can be displayed using \r\n\r\n```sh\r\n$ ./ivm display unitsquaregp.model\r\n```\r\n\r\n```sh\r\nLoading model file.\r\n... done.\r\nIVM Model:\r\nActive Set Size: 200\r\nKernel Type:\r\ncompound kernel:\r\nrbfinverseWidth: 12.1211\r\nrbfvariance: 0.136772\r\nbiasvariance: 0.000229177\r\nwhitevariance: 0.0784375\r\nNoise Type:\r\nProbit noise:\r\nBias on process 0: 0.237516\r\n```\r\n\r\nNotice the fact that the kernel is composed of an RBF kernel, also known as squared exponential kernel or Gaussian kernel; a bias kernel, which is just a constant, and a white noise kernel, which is a diagonal term. The bias kernel and the white kernel are automatically added to the rbf kernel. Other kernels may also be used, see `ivm learn -h` for details.\r\n\r\nFor this model the input data is two dimensional, you can therefore visualise the decision boundary using\r\n\r\n```sh\r\n$ ./ivm gnuplot examples/unitsquaregp.svml unitsquaregp.model unitsquaregp\r\n```\r\n\r\nThe `unitsquaregp` supplied as the last argument acts as a stub for gnuplot to create names from, so for example (using gnuplot vs 4.0 or above), you can write\r\n\r\n```sh\r\n$ gnuplot unitsquaregp_plot.gp\r\n```\r\n\r\nand obtain the plot shown below\r\n<center><img src=\"unitsquaregp_plot.png\"><br>\r\nThe decision boundary learnt for the data sampled from a Gaussian process classification. Note the active points (blue stars) typically lie along the decision boundary.</center><br>\r\n\r\nThe other files created are `oil100_variance_matrix.dat`, which produces the grayscale map of the log precisions and `oil100_latent_data1-3.dat` which are files containing the latent data positions associated with each label.\r\n\r\n### Feature Selection\r\n\r\nNext we consider a simple ARD kernel. The toy data in this case is sampled from three Gaussian distributions. To separate the data only one input dimension is necessary. The command is run as follows,\r\n\r\n```sh\r\n$ ./ivm learn -a 100 -k rbf -i 1 examples/ard_gaussian_clusters.svml ard_gaussian_clusters.model\r\n```\r\n\r\nDisplaying the model it is clear that it has selected one of the input dimensions, \r\n\r\n```sh\r\nLoading model file.<br>\r\n... done.<br>\r\nIVM Model:<br>\r\nActive Set Size: 100<br>\r\nKernel Type:<br>\r\ncompound kernel:<br>\r\nrbfardinverseWidth: 0.12293<br>\r\nrbfardvariance: 2.25369<br>\r\nrbfardinputScale: 5.88538e-08<br>\r\nrbfardinputScale: 0.935148<br>\r\nbiasvariance: 9.10663e-07<br>\r\nwhitevariance: 2.75252e-08<br>\r\nNoise Type:<br>\r\nProbit noise:<br>\r\nBias on process 0: 0.745098\r\n\r\n```\r\n\r\nOnce again the results can be displayed as a two dimensional plot,\r\n\r\n```sh\r\n$ ./ivm gnuplot examples/ard_gaussian_clusters.svml ard_gaussian_clusters.model ard_gaussian_clusters\r\n```\r\n\r\n<center><img src=\"ard_gaussian_clusters_plot.png\"><br>\r\nThe IVM learnt with an ARD RBF kernel. One of the input directions has been recognised as not relevant.\r\n</center>\r\n\r\n\r\n## Semi-Supervised Learning\r\n\r\nThe software also provides an implementation of the null category noise model described in <a href=\"http://ml.sheffield.ac.uk/~neil/cgi-bin/publications/bibpage.cgi?keyName=Lawrence:semisuper04\">Lawrence and Jordan</a>. \r\n\r\nThe toy example given in the paper is reconstructed here. To run it type\r\n\r\n```sh\r\n$ ./ivm learn -a 100 -k rbf examples/semisupercrescent.svml semisupercrescent.model\r\n```\r\n\r\nThe result of learning is\r\n\r\n```sh\r\nLoading model file.\r\n... done.\r\nIVM Model:\r\nActive Set Size: 100\r\nKernel Type:\r\ncompound kernel:\r\nrbfinverseWidth: 0.0716589\r\nrbfvariance: 2.58166\r\nbiasvariance: 2.03635e-05\r\nwhitevariance: 3.9588e-06\r\nNoise Type:\r\nNcnm noise:\r\nBias on process 0: 0.237009\r\nMissing label probability for -ve class: 0.9075\r\nMissing label probability for +ve class: 0.9075\r\n```\r\n\r\nand can be visualised using\r\n\r\n```sh\r\n$ ./ivm gnuplot examples/semisupercrescent.svml semisupercrescent.model semisupercrescent\r\n```\r\n\r\nfollowed by \r\n\r\n```sh\r\n$ gnuplot semisupercrescent_plot.gp\r\n```\r\n\r\nThe result of the visualisation being,\r\n\r\n<center><img src=\"semisupercrescent_plot.png\"><img src=\"semisupercrescent_labels_only_plot.png\"><br>The result of semi-supervised learning on the crescent data. At the top is the result from the null category noise model. The bottom shows the result from training only on the labelled data only with the standard probit noise model. Purple squares are unlabelled data, blue stars are the active set. <center>\r\n\r\n\r\nGP-LVM Software\r\n===============\r\n\r\nThis page describes how to compile and gives some examples of use of the C++ Gaussian Process Latent Variable Model Software (GP-LVM) available for <a href=\"http://ml.sheffield.ac.uk/~neil/cgi-bin/software/downloadForm.cgi?toolbox=gplvmcpp\">download here</a>. \r\n\r\n### Release Information\r\n\r\n#### Release 0.201\r\n\r\nFixed bug which meant that back constraint wasn't working due to failure to initialise lwork properly for dsysv. \r\n\r\nFixed bug in gplvm.cpp which meant dynamics wasn't working properly because initialization of dynamics model learning parameter wasn't set to zero.\r\n\r\nThanks to Gustav Henter for pointing out these problems.\r\n\r\n#### Release 0.2\r\n\r\nIn this release we updated the class structure of the gplvm model and\r\nmade some changes in the way in which files are stored. This release\r\nis intended as a stopgap before a release version in which fitc, dtc\r\nand variational dtc approximations will be available.\r\n\r\nIn this release the dynamics model of <a href=\"http://www.dgp.toronto.edu/~jmwang/gpdm/\">Wang <i>et al</i>.</a> has been included. The initial work was done by William V. Baxter, with modifications by me to include the unusual prior Wang suggests in his MSc thesis, scaling of the dynamics likelihood and the ability to set the signal noise ratio. A new example has been introduced for this model below.\r\n\r\nAs part of the dynamics introduction a new MATLAB toolbox for the GP-LVM has been released. This toolbox, <a href=\"http://ml.sheffield.ac.uk/~neil/cgi-bin/software/downloadForm.cgi?toolbox=fgplvm\">download here</a>, is expected to be the main development toolbox for the GP-LVM in MATLAB.\r\n\r\nVersion 0.101 was released 21st October 2005.\r\n\r\nThis release contained modifications by William V. Baxter to enable the code to work with Visual Studio 7.1.\r\n\r\nVersion 0.1, was released in late July 2005.\r\n\r\nThis was the original release of the code.\r\n\r\n### Design Philosophy\r\n\r\nThe software is written in C++ to try and get a degree of\r\nflexibility in the models that can be used without a serious\r\nperformance hit. This was difficult to do in MATLAB as users who have\r\ntried version 1 (which was fast but inflexible) and version 2 (which\r\nwas flexible but slow) of the MATLAB software will appreciate.\r\n\r\nThe sparsification algorithm has not been implemented in the C++\r\nsoftware so this software is mainly for smaller data sets (up to\r\naround a thousand points).\r\n\r\nThe software is mainly written in C++ but relies for some functions\r\non FORTRAN code by other authors and the LAPACK and BLAS libraries.\r\n\r\nAs well as the C++ code some utilities are supplied in MATLAB code\r\nfor visualising the results.\r\n\r\n## Compiling the Software\r\n\r\nThe software was written with gcc vs 3.2.2. There are definitely\r\nStandard Template Library issues on Solaris with gcc 2.95, so I\r\nsuggest that at least version 3.2 or above is used.\r\n\r\nPart of the reason for using gcc is the ease of interoperability\r\nwith FORTRAN. The code base makes fairly extensive use of FORTRAN so\r\nyou need to have g77 installed.  The software is compiled by\r\nwriting\r\n\r\n```sh\r\n$ make gplvm\r\n```\r\n\r\nat the command line. Architecture specific options are included in\r\nthe `make.ARCHITECTURE` files. Rename the file with the\r\nrelevant architecture to `make.inc` for it to be included.\r\n\r\n### Optimisation\r\n\r\nOne of the advantages of interfacing to the LAPACK and BLAS libraries\r\nis that they are often optimised for particular architectures. The\r\nfile `make.atlas` includes options for compiling the ATLAS\r\noptimised versions of lapack and blas that are available on a server I\r\nhave access to. These options may vary for particular machines.\r\n\r\n### Cygwin\r\n\r\nFor Windows users the code compiles under cygwin. However you will\r\nneed version s of the lapack and blas libraries available (see <a\r\nhref=\"http://www.netlib.org\">www.netlib.org</a>. This can take some\r\ntime to compile, and in the absence of any pre-compiled versions on\r\nthe web I've provided some pre-compiled versions you may want to make\r\nuse of (see the cygwin directory). Note that these pre-compiled\r\nversions are <i>not</i> optimised for the specific architecture and\r\ntherefore do not give the speed up you would hope for from using\r\nlapack and blas.\r\n\r\n### Microsoft Visual C++\r\n\r\nAs of Release 0.101 the code compiles under Microsoft Visual Studio\r\n7.1. A project file is provided in the current release in the\r\ndirectory `MSVC/gplvm`. The compilation makes use of f2c\r\nversions of the FORTRAN code and the C version of LAPACK/BLAS,\r\nCLAPACK. Detailed instructions on how to compile are in the\r\nreadme.msvc file. The work to convert the code (which included ironing\r\nout several bugs) was done by William V. Baxter. Many thanks to Bill\r\nfor allowing me to make this available.\r\n\r\n## General Information\r\n\r\nThe way the software operates is through the command line. There is\r\none executable, `gplvm`. Help can be obtained by writing\r\n\r\n```sh\r\n$ ./gplvm -h\r\n```\r\n\r\nwhich lists the commands available under the software. Help for\r\neach command can then be obtained by writing, for example,\r\n\r\n```sh\r\n$ ./gplvm learn -h\r\n```\r\n\r\nAll the tutorial optimisations suggested take less than 1/2 hour to\r\nrun on my less than 2GHz Pentium IV machine. The first oil example\r\nruns in a couple of minutes. Below I suggest using the highest\r\nverbosity options `-v 3` in each of the examples so that\r\nyou can track the iterations.\r\n\r\n## Examples\r\n\r\nThe software loads in data in the <a\r\nhref=\"http://svmlight.joachims.org/\">SVM light</a> format. This is to\r\nprovide compatibility with other <a href=\"https://github.com/SheffieldML/GPmat/\">Gaussian\r\nProcess software</a>. Anton Schwaighofer has written <a\r\nhref=\"http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/software/svml_toolbox.html\"> a package</a>\r\nwhich can write from MATLAB to the SVM light format.\r\n\r\n## Oil Flow Data\r\n\r\nIn the original NIPS paper the first example was the oil flow data\r\n(see <a href=\"http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/3PhaseData.html\">this\r\npage</a> for details) sub-sampled to 100 points. I use this data a lot\r\nfor checking the algorithm is working so in some senses it is not an\r\nindependent `proof' of the model.\r\n\r\nProvided with the software, in the `examples` directory,\r\nis a sub-sample of the oil data. The file is called\r\n`oilTrain100.svml`.\r\n\r\nFirst we will learn the data using the following command,\r\n\r\n```sh\r\n$ ./gplvm -v 3 learn -# 100 examples/oilTrain100.svml oil100.model\r\n```\r\n\r\nThe flag `-v 3` sets the verbosity level to 3 (the\r\nhighest level) which causes the iterations of the scaled conjugate\r\ngradient algorithm to be shown. The flag `-# 100`\r\nterminates the optimisation after 100 iterations so that you can\r\nquickly move on with the rest of the tutorial.\r\n\r\nThe software will load the data in\r\n`oilTrain100.svml`. The labels are included in this file\r\nbut they are <i>not</i> used in the optimisation of the model. They\r\nare for visualisation purposes only.\r\n\r\n### Gnuplot\r\n\r\nThe learned model is saved in a file called\r\n`oil100.model`. This file has a plain text format to make\r\nit human readable. Once training is complete, the learned kernel\r\nparameters of the model can be displayed using\r\n\r\n```sh\r\n$ ./gplvm display oil100.model\r\n```\r\n\r\n```sh\r\n\r\nLoading model file.\r\n... done.\r\nGPLVM Model:\r\nData Set Size: 100\r\nKernel Type:\r\ncompound kernel:\r\nrbfinverseWidth: 3.97209\r\nrbfvariance: 0.337566\r\nbiasvariance: 0.0393251\r\nwhitevariance: 0.00267715\r\n```\r\n\r\nNotice the fact that the kernel is composed of an RBF kernel, also\r\nknown as squared exponential kernel or Gaussian kernel; a bias kernel,\r\nwhich is just a constant, and a white noise kernel, which is a\r\ndiagonal term. This is the default setting, it can be changed with\r\nflags to other kernel types, see `gplvm learn -h` for\r\ndetails.\r\n\r\nFor your convenience a `gnuplot` file may generated to\r\nvisualise the data. First run\r\n\r\n```sh\r\n$ ./gplvm gnuplot oil100.model oil100\r\n```\r\n\r\nThe `oil100` supplied as the last argument acts as a\r\nstub for gnuplot to create names from, so for example (using gnuplot\r\nvs 4.0 or above), you can write\r\n\r\n```sh\r\n$ gnuplot oil100_plot.gp\r\n```\r\n\r\nAnd obtain the plot shown below\r\n<center><img src=\"oil100_plot.png\"><br>\r\nVisualisation of 100 points of the oil flow data.</center><br>\r\n\r\nThe other files created are\r\n`oil100_variance_matrix.dat`, which produces the grayscale\r\nmap of the log precisions and `oil100_latent_data1-3.dat`\r\nwhich are files containing the latent data positions associated with\r\neach label.\r\n\r\n### The Entire Oil Data Set\r\n\r\nRunning the GPLVM for 1000 iterations on all 1000 points of the oil\r\ndata leads to the visualisation below.\r\n\r\n<center><img src=\"oil1000_plot.png\"><br>\r\nAll 1000 points of the oil data projected into latent space. This visualisation takes overnight to optimise on a Pentinum IV.\r\n</center>\r\n\r\n### MATLAB\r\n\r\nWhile MATLAB can be horribly slow (and very expensive for non-academic\r\nusers) it is still a lot easier (for me) to code the visualisation\r\nroutines by building on MATLAB's graphics facilities. To this end a\r\nnew release of the GPLVM code in MATLAB has been provided (vs 2.012\r\nand above) which allows you to load the results of the learning from\r\nthe C++ code into MATLAB for further manipulation. You can download\r\nthe toolbox from <a\r\nhref=\"http://ml.sheffield.ac.uk/~neil/cgi-bin/software/downloadForm.cgi?toolbox=gplvm\">here</a>. Once\r\nthe relevant toolboxes (you need the IVM toolbox and the toolboxes on\r\nwhich it depends: KERN, NOISE, etc.) are downloaded you can visualise\r\nthe results in MATLAB using\r\n\r\n```matlab\r\n>> gplvmResultsCpp('oil100.model', 'vector')\r\n>>\r\n```\r\n\r\nThis will load the results and allow you to move around the latent\r\nspace visualising (in the form of a line plotted from the vector) the\r\nnature of the data at each point.\r\n\r\n## Motion Capture\r\n\r\nOne popular use of the GPLVM has been in learning of human motion\r\nstyles (see <a\r\nhref=\"http://ml.sheffield.ac.uk/~neil/cgi-bin/publications/bibpage.cgi?keyName=Grochow:styleik04&printAbstract=1\">Grochow\r\n<i>et al.</i></a>). Personally, I find this application very\r\nmotivating as Motion Capture data is a rare example of high\r\ndimensional data about which humans have a strong intuition. If the\r\nmodel fails to model `natural motion' it is quite apparant to a human\r\nobserver. Therefore, as a second example, we will look at data of this\r\ntype. In particular we will consider a data sets containing a walking\r\nman and a further data set containing a horse. To run these demos you\r\nwill also need a small <a\r\nhref=\"http://ml.sheffield.ac.uk/~neil/cgi-bin/software/downloadForm.cgi?toolbox=mocap\">MATLAB\r\nmocap toolkit</a>.\r\n\r\n### BVH Files\r\n\r\nTo prepare a new bvh file for visualisation you need the MATLAB\r\nmocap toolkit and Anton Schwaighofer's <a\r\nhref=\"http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/software/svml_toolbox.html\"> SVM light\r\nMATLAB interface</a> (you don't need the SVM light software itself).\r\n\r\n```matlab\r\n>> [bvhStruct, channels, frameLength] = bvhReadFile('examples/Swagger.bvh');\r\n>>\r\n```\r\n\r\nThis motion capture data was taken from Ohio State University's <a\r\nhref=\"http://accad.osu.edu/research/mocap/mocap_data.htm\">ACCAD</a>\r\ncentre.\r\n\r\nThe motion capture channels contain values for the offset of the\r\nroot node at each frame. If we don't want to model this motion it can\r\nbe removed at this stage. Setting the 1st, 3rd and 6th channels to\r\nzero removes X and Z position and the rotation in the Y plane.\r\n\r\n```matlab\r\n>> channels(:, [1 3 6]) = zeros(size(channels, 1), 3);\r\n```\r\n\r\nYou can now play the data using the command\r\n\r\n```matlab\r\n>> bvhPlayData(bvhStruct, channels, frameLength);\r\n```\r\n\r\nData in the bvh format consists of angles, this presents a problem\r\nwhen the angle passes through a discontinuity. For example in this\r\ndata the 'lhumerus' and 'rhumerus' joints rotate through 180 degrees\r\nand the channel moves from -180 to +180. This arbitrary difference\r\nwill seriously effect the results. The fix is to add or subtract 360\r\nas appropriate. In the toolbox provided this is done automatically in\r\nthe file bvhReadFile.m using the function channelsAngles. This works\r\nwell for the files we use here, but may not be a sufficient solution\r\nfor files with more rotation on the joints.\r\n\r\nThen channels can be saved for modelling using Schwaighofer's SVM\r\nlight interface. First we downsample so that things run quickly,\r\n\r\n```matlab\r\n>> channels = channels(1:4:end, :);\r\n```\r\n\r\nThen the data is saved as follows:\r\n\r\n```matlab\r\n>> svmlwrite('examples/swagger.svml',channels)\r\n```\r\n\r\n<i>Before you save you might want to check you haven't messed\r\nanything up by playing the data again!</i> It makes sense to learn\r\nthe scale independently for each the channels (particularly since we\r\nhave set three of them to zero!), so we now use the gplvm code to\r\nlearn the data setting the flag `-L true` for learning of\r\nscales.\r\n\r\n```sh\r\n$ ./gplvm -v 3 learn -L true examples/swagger.svml swagger.model\r\n```\r\n\r\nOnce learning is complete the results can be visualised in MATLAB\r\nusing the command\r\n\r\n```matlab\r\n>> mocapResultsCppBvh('swagger.model', 'examples/Swagger.bvh', 'bvh');\r\n```\r\n\r\n<center><img src=\"swagger_plot.png\">\r\n<br>Latent space for the Swagger data. Note the breaks in the sequence.</center>\r\n\r\n### Dealing with the Breaks\r\n\r\nNote that there are breaks in the sequence. These reason for these\r\nbreaks is as follows. The GPLVM maps from the latent space to the data\r\nspace with a smooth mapping. This means that points that are nearby in\r\nlatent space will be nearby in data space. However that does not imply\r\nthe reverse, i.e. points that are nearby in data space will not\r\nnecessarily be nearby in latent space. It implies that if points are\r\nfar apart in data space they will be far apart in latent space which\r\nis a slightly different thing. This means that the model is not\r\nstrongly penalised for breaking the sequence (even if a better\r\nsolution can be found through not breaking the sequence).\r\n\r\nFor visualisation you often want points being nearby in data space to\r\nbe nearby in latent space. For example, most of the recent spectral\r\ntechniques (including kernel PCA, Isomap, and LLE) try and guarantee\r\nthis. In recent unpublished work with <a\r\nhref=\"http://www.kyb.tuebingen.mpg.de/~jqc\">Joaquin Quinonero\r\nCandela</a>, we have shown that this can be achieved in the GPLVM\r\nusing `back constraints'. We constrain the data in the latent space to\r\nbe represented by a second reverse-mapping from the data space. For\r\nthe walking man the show results you can test the back constraints\r\nwith the command\r\n\r\n\r\n```sh\r\n$ ./gplvm -v 3 learn -L true -c rbf -g 0.0001 examples/swagger.svml swagger_back_constrained.model\r\n```\r\n\r\n\r\nThe back constraint here is a kernel mapping with an `RBF' kernel\r\nwhich is specified as having an inverse width of 1e-4.\r\n \r\nThe results can then be seen in MATLAB using\r\n\r\n```matlab\r\n>> mocapResultsCppBvh('swagger_back_constrained.model', 'examples/Swagger.bvh', 'bvh');\r\n```\r\n\r\n<center><img src=\"swagger_back_constrained_plot.png\">\r\n<br>The repeated circular pattern is associated with the repeated walking paces in the data.</center>\r\n\r\n### Dealing with the Breaks with Dynamics\r\n\r\nIt conceptually straightforward to add MAP dynamics in the GP-LVM\r\nlatent space by placing a prior that relates the latent points\r\ntemporally. There are several ways one could envisage doing this. <a\r\nhref=\"http://www.dgp.toronto.edu/~jmwang/gpdm/\">Wang <i>et al</i></a>\r\nproposed introducing dynamics through the use of a Gaussian process\r\nmapping across time points. William V. Baxter implemented this\r\nmodification to the code and kindly allowed me to make his\r\nmodifications available. In the base case adding dynamics associated\r\nwith a GP doesn't change things very much: the Gaussian process\r\nmapping is too flexible and doesn't constrain the behaviour of the\r\nmodel. To solve this problem Wang suggests using a particular prior on\r\nthe hyper parameters of the GP-LVM (see pg 58 of his <a\r\nhref=\"http://www.dgp.toronto.edu/~jmwang/gpdmthesis.pdf\">Master's\r\nthesis</a>). This prior is unusual as it is improper, but it is not\r\nthe standard uninformative 1/x prior. This approach can be recreated\r\nusing the `-dh` flag when running the code. An alternative\r\napproach of scaling the portion of the likelihood associated with the\r\ndynamics up by a factor has also been suggested. This approach can be\r\nrecreated by using the `-ds` flag.\r\n\r\nMy own preference is to avoid either of these approaches. A key\r\nmotivation of the GP-LVM as a probabilistic model was to design an\r\napproach that avoided difficult to justify scalings and unusual\r\npriors. The basic problem is that if the hyper parameters are\r\noptimised the Gaussian process is too flexible a model for application\r\nmodelling the dynamics. However it is also true that a non-linear\r\nmodel is needed. As an alternative approach we suggest fixing the\r\nhyper parameters. The level of noise can be fixed by suggesting a\r\nsignal to noise ratio. This approach has also been implemented in the\r\ncode using the `-dr` flag.\r\n\r\n```sh\r\n$ ./gplvm -v 3 learn -L true -D rbf -g 0.01 -dr 20 examples/swagger.svml swagger_dynamics.model\r\n```\r\n\r\nwhere the `-M` flag sets the parameter associated with\r\nWang's prior. Here the dynamics GP is given a linear and an RBF\r\nkernel. The results of the visualisation are shown below.\r\n\r\n<center><img src=\"swagger_dynamics_plot.png\"> <br>Latent space for\r\nthe Swagger data with the dynamics. By constraining the GP-LVM with an\r\nunusual prior the sequence stays continuous in latent space.</center>\r\n\r\nThis result can also be loaded into MATLAB and played using the command \r\n\r\n```matlab\r\n>> mocapResultsCppBvh('swagger_dynamics.model', 'examples/Swagger.bvh', 'bvh');\r\n```\r\n\r\nMatlab mex-files\r\n================\r\n\r\nIt is possible to use GPc in Matlab using a mex-file, combining the speed of\r\nC++ and comfort of Matlab. However, there are several issues which might need\r\na special attention:\r\n\r\n* when getting errors about symbols _defined in discarded section_, it\r\nsurprisingly might help to remove the Matlab interface (comment the\r\nMatlab-related lines in your makefile.platform out);\r\n\r\n* when getting segmentation faults in calls to BLAS/LAPACK libraries, it might\r\nhelp to compile the mex-file against MKL library instead (use -lmkl_rt instead\r\nof -lblas -llapack).\r\n\r\n\r\n","google":"UA-62971049-1","note":"Don't delete this file! It's used internally to help with page regeneration."}